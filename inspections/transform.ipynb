{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data_raw = '../nosync/data_raw'\n",
    "data_csv_only_converted = '../nosync/data_csv_full_only_converted'\n",
    "data_csv_transformed = '../nosync/data_csv'\n",
    "data_csv_reduced = '../nosync/data_csv_reduced'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert XML to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "def transform(file):\n",
    "    input_file = file\n",
    "\n",
    "    #removing newlines\n",
    "    clean = open(input_file, encoding='utf8').read().replace('\\n', '')\n",
    "    f = open(input_file, 'w', encoding='utf8')\n",
    "    f.write(clean)\n",
    "    f.close()\n",
    "\n",
    "    STK = []\n",
    "    DrTP = []\n",
    "    VIN = []\n",
    "    DatKont = []\n",
    "    TZn = []\n",
    "    TypMot = []\n",
    "    DrVoz = []\n",
    "    ObchOznTyp = []\n",
    "    Ct = []\n",
    "    DatPrvReg = []\n",
    "    Km = []\n",
    "    Zavady = []\n",
    "    tmp = []\n",
    "    VyslSTK = []\n",
    "    VyslEmise = []\n",
    "\n",
    "    # parsing\n",
    "    parser = ET.iterparse(input_file)\n",
    "    for event, element in parser:\n",
    "        if element.tag == 'record':\n",
    "\n",
    "            # Number of STK (station itself)\n",
    "            if 'STK' in element.attrib:\n",
    "                STK.append(element.attrib['STK'])\n",
    "            elif 'CisP' in element.attrib:\n",
    "                STK.append(element.attrib['CisP'].split('-')[1])\n",
    "            else:\n",
    "                STK.append('')\n",
    "\n",
    "            if 'DrTP' in element.attrib:\n",
    "                DrTP.append(element.attrib['DrTP'])\n",
    "            else:\n",
    "                DrTP.append('')\n",
    "\n",
    "            if 'VIN' in element.attrib:\n",
    "                VIN.append(element.attrib['VIN'])\n",
    "            else:\n",
    "                VIN.append('')\n",
    "\n",
    "            if 'DatKont' in element.attrib:\n",
    "                DatKont.append(element.attrib['DatKont'])\n",
    "            else:\n",
    "                DatKont.append('')\n",
    "\n",
    "            if 'TZn' in element.attrib:\n",
    "                TZn.append(element.attrib['TZn'])\n",
    "            else:\n",
    "                TZn.append('')\n",
    "\n",
    "            if 'TypMot' in element.attrib:\n",
    "                TypMot.append(element.attrib['TypMot'])\n",
    "            else:\n",
    "                TypMot.append('')\n",
    "\n",
    "            if 'DrVoz' in element.attrib:\n",
    "                DrVoz.append(element.attrib['DrVoz'])\n",
    "            else:\n",
    "                DrVoz.append('')\n",
    "\n",
    "            if 'ObchOznTyp' in element.attrib:\n",
    "                ObchOznTyp.append(element.attrib['ObchOznTyp'])\n",
    "            else:\n",
    "                ObchOznTyp.append('')\n",
    "\n",
    "            if 'Ct' in element.attrib:\n",
    "                Ct.append(element.attrib['Ct'])\n",
    "            else:\n",
    "                Ct.append('')\n",
    "\n",
    "            if 'DatPrvReg' in element.attrib:\n",
    "                DatPrvReg.append(element.attrib['DatPrvReg'])\n",
    "            else:\n",
    "                DatPrvReg.append('')\n",
    "\n",
    "            if 'Km' in element.attrib:\n",
    "                Km.append(element.attrib['Km'])\n",
    "            else:\n",
    "                Km.append('')\n",
    "\n",
    "            # There are no defects recorded in the 2018 dataset, so this is useless.\n",
    "            # In the newer datasets, it's computed after conversion to dataframe from the raw \"Zav\" field.\n",
    "\n",
    "            # if 'ZavA' in element.attrib:\n",
    "            #     tmp.append(element.attrib['ZavA'])\n",
    "            # else:\n",
    "            #     tmp.append('')\n",
    "\n",
    "            # if 'ZavB' in element.attrib:\n",
    "            #     tmp.append(element.attrib['ZavB'])\n",
    "            # else:\n",
    "            #     tmp.append('')\n",
    "\n",
    "            # if 'ZavC' in element.attrib:\n",
    "            #     tmp.append(element.attrib['ZavC'])\n",
    "            # else:\n",
    "            #     tmp.append('')\n",
    "\n",
    "            if 'Zav' in element.attrib:\n",
    "                Zavady.append(element.attrib['Zav'])\n",
    "            else:\n",
    "                Zavady.append(','.join(tmp))\n",
    "                tmp = []\n",
    "\n",
    "            if 'VyslSTK' in element.attrib:\n",
    "                VyslSTK.append(element.attrib['VyslSTK'])\n",
    "            elif 'Vysl' in element.attrib:\n",
    "                VyslSTK.append(element.attrib['Vysl'])\n",
    "            else:\n",
    "                VyslSTK.append('')\n",
    "\n",
    "            if 'VyslEmise' in element.attrib:\n",
    "                VyslEmise.append(element.attrib['VyslEmise'])\n",
    "            else:\n",
    "                VyslEmise.append('')\n",
    "\n",
    "            element.clear()\n",
    "\n",
    "    data = pd.DataFrame({'STK': STK, 'DrTP': DrTP, 'VIN': VIN, 'DatKont': DatKont, 'TypMot': TypMot, 'TZn': TZn, 'DrVoz': DrVoz, 'ObchOznTyp': ObchOznTyp, 'Ct': Ct, 'DatPrvReg': DatPrvReg, 'Km': Km, 'Zavady': Zavady, 'VyslSTK': VyslSTK, 'VyslEmise': VyslEmise})\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Load raw xmls\n",
    "dirs_raw = {}\n",
    "\n",
    "for dir in os.listdir(data_raw):\n",
    "    if not os.path.isdir(data_raw + '/' + dir):\n",
    "        continue\n",
    "    for instance in os.listdir(data_raw + '/'  + dir):\n",
    "        year = instance.split('.')[0].split('_')[2]\n",
    "        month = instance.split('.')[0].split('_')[3]\n",
    "        print(f'{year}-{month}')\n",
    "        data = transform(f'{data_raw}/{dir}/{instance}')\n",
    "        data.to_csv(f'{data_csv_only_converted}/{year}-{month}.csv')\n",
    "        # instances.append(instance)\n",
    "    # dirs_raw[dir] = instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle 2018 separately to split months.\n",
    "# This needs a few gigs of RAM.\n",
    "all_data = transform(f'{data_raw}/Seznam_prohliÃÅdek_STK_2018.xml')\n",
    "for month in range(1, 13):\n",
    "    datepadded = f'2018-{str(month).rjust(2, \"0\")}'\n",
    "    # Flag rows from the current month.\n",
    "    all_data['retain'] = all_data['DatKont'].apply(lambda val: True if val.startswith(datepadded) else False)\n",
    "    # Save positively flagged rows without the flag column\n",
    "    all_data[all_data['retain']].loc[:, all_data.columns != 'retain'].to_csv(f'{data_csv_only_converted}/{datepadded}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unify and preprocess column values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the processing pipeline\n",
    "\n",
    "dataset_filenames = [filename for filename in os.listdir(data_csv_only_converted)]\n",
    "pipelines = {filename: [] for filename in dataset_filenames}\n",
    "\n",
    "# def apply_func(dataset_path, func, tmp=None):\n",
    "#     \"\"\"Apply a single function to a CSV dataset. Inefficient loading, use this only for testing.\"\"\"\n",
    "#     df = pd.read_csv(dataset_path, index_col=0)\n",
    "#     x = dataset_path.split('/')[-1].split('.')[-2].split('-')\n",
    "#     year = x[0]\n",
    "#     month = x[1]\n",
    "#     df = func(df)\n",
    "\n",
    "#     if tmp == 'tmp':\n",
    "#         df.to_csv(dataset_path + '_tmp.csv')<\n",
    "#     df.to_csv(dataset_path)\n",
    "\n",
    "# # For mem only computations\n",
    "# datasets = [pd.read_csv(path, index_col=0) for path in dataset_filenames]\n",
    "# def apply_func_to_all(func, tmp = 'mem'):\n",
    "#     for df in datasets:\n",
    "#         df = func(df)\n",
    "\n",
    "def apply_pipeline(dataset_filename, pipeline):\n",
    "    \"\"\"Apply a list of functions to a CSV dataset\"\"\"\n",
    "    df = pd.read_csv(data_csv_only_converted + '/' + dataset_filename, index_col=0)\n",
    "    # x = dataset_filename.split('.')[-2].split('-')\n",
    "    # year = x[0]\n",
    "    # month = x[1]\n",
    "    for func in pipeline:\n",
    "        print('  ' + func.__name__)\n",
    "        df = func(df)\n",
    "    df.to_csv(data_csv_transformed + '/' + dataset_filename)\n",
    "\n",
    "def append_pipeline_for_all(func):\n",
    "    for dataset_filename in dataset_filenames:\n",
    "        pipelines[dataset_filename].append(func)\n",
    "\n",
    "def run():\n",
    "    for dataset_filename, pipeline in pipelines.items():\n",
    "        print(dataset_filename)\n",
    "        apply_pipeline(dataset_filename, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace `---` with np.nan\n",
    "###### Drop records with missing essential values.\n",
    "\n",
    "def process_missing(df):\n",
    "    return df.replace('---', np.NaN)\n",
    "    return df.replace('', np.NaN)\n",
    "\n",
    "append_pipeline_for_all(process_missing)\n",
    "# apply_func_to_all(process_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split DatKont and DatPrvReg to separate year, month, day columns. Add weekday indicator.\n",
    "# Convert dates to a pandas date.\n",
    "# 2018-01 => 2019-03  yyyy-mm-ddThh:mm:ss.sss\n",
    "# 2019-04 => 2019-07  mm/dd/yyyy\n",
    "# 2019-08+            dd.mm.yyyy\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "def add_weekday(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['DTKont'] = df['DatKont'].apply(lambda date: date.isoweekday())\n",
    "    return df\n",
    "\n",
    "def strip_time_v1(df):\n",
    "    # df[['RokKont', 'MesKont', 'DenKont']] = df['DatKont'].str.split('T').str[0].str.split('-', expand=True).astype('uint16')\n",
    "    # df[['HodKont', 'MinKont']] = df['DatKont'].str.split('T').str[1].str.split(':', expand=True).iloc[:, :2].astype('uint8')\n",
    "    df['DatKont'] = pd.to_datetime(df['DatKont'], format = '%Y-%m-%d', exact = False, errors = 'coerce')\n",
    "    # df[['RokPrvReg', 'MesPrvReg', 'DenPrvReg']] = df['DatPrvReg'].str.split('T').str[0].str.split('-', expand=True).astype('uint16')\n",
    "    df['DatPrvReg'] = pd.to_datetime(df['DatPrvReg'], format = '%Y-%m-%d', exact = False, errors = 'coerce')\n",
    "    return add_weekday(df)\n",
    "\n",
    "def strip_time_v2(df):\n",
    "    # df[['MesKont', 'DenKont', 'RokKont']] = df['DatKont'].str.split('/', expand=True).astype('uint16')\n",
    "    df['DatKont'] = pd.to_datetime(df['DatKont'], format = '%m/%d/%Y', exact = True, errors = 'coerce')\n",
    "    # df[['MesPrvReg', 'DenPrvReg', 'RokPrvReg']] = df['DatPrvReg'].str.split('/', expand=True).astype('uint16')\n",
    "    df['DatPrvReg'] = pd.to_datetime(df['DatPrvReg'], format = '%m/%d/%Y', exact = True, errors = 'coerce')\n",
    "    return add_weekday(df)\n",
    "\n",
    "def strip_time_v3(df):\n",
    "    # df[['DenKont', 'MesKont', 'RokKont']] = df['DatKont'].str.split('.', expand=True).astype('uint16')\n",
    "    df['DatKont'] = pd.to_datetime(df['DatKont'], format = '%d.%m.%Y', exact = True, errors = 'coerce')\n",
    "    # df[['DenPrvReg', 'MesPrvReg', 'RokPrvReg']] = df['DatPrvReg'].str.split('.', expand=True).astype('uint16')\n",
    "    df['DatPrvReg'] = pd.to_datetime(df['DatPrvReg'], format = '%d.%m.%Y', exact = True, errors = 'coerce')\n",
    "    return add_weekday(df)\n",
    "\n",
    "# Add appropriate function to each dataset's pipeline\n",
    "for dfn in dataset_filenames:\n",
    "    if dfn.__contains__('2018') or dfn.__contains__('2019-01') \\\n",
    "        or dfn.__contains__('2019-02') or dfn.__contains__('2019-03'):\n",
    "        pipelines[dfn].append(strip_time_v1)\n",
    "    elif dfn.__contains__('2019-04') or dfn.__contains__('2019-05') \\\n",
    "        or dfn.__contains__('2019-06') or dfn.__contains__('2019-07'):\n",
    "        pipelines[dfn].append(strip_time_v2)\n",
    "    else:\n",
    "        pipelines[dfn].append(strip_time_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add counts of failures by type.\n",
    "\n",
    "# Load ciselnik_zavad.\n",
    "cz = pd.read_csv('../../defect_list/defect_list.csv')\n",
    "cz.set_index('Kod', inplace=True)\n",
    "# display(cz.head())\n",
    "\n",
    "def count_failures(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\"Add ZavA, ZavB, ZavC (count of failures by severity) based on the contents of `Zav`\"\"\"\n",
    "\n",
    "    def map_failures(row):\n",
    "        failure_codes = str(row['Zavady']).split(',')\n",
    "        if len(failure_codes) == 0:\n",
    "            return (0, 0, 0)\n",
    "        else:\n",
    "            a = 0\n",
    "            b = 0\n",
    "            c = 0\n",
    "            for failure_code in failure_codes:\n",
    "                type = None\n",
    "                try:\n",
    "                    type = cz['Typ'].loc[failure_code]\n",
    "                except:\n",
    "                    pass\n",
    "                if type == 'A':\n",
    "                    a = a + 1\n",
    "                if type == 'B':\n",
    "                    b = b + 1\n",
    "                if type == 'C':\n",
    "                    c = c + 1\n",
    "            return (a, b, c)\n",
    "   \n",
    "    df[['ZavA', 'ZavB', 'ZavC']] = df.apply(map_failures, axis=1, result_type='expand')\n",
    "    return df\n",
    "\n",
    "append_pipeline_for_all(count_failures)\n",
    "# apply_func_to_all(count_failures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add counts of failures by first-level category\n",
    "\n",
    "def count_failures_by_fl_cat(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\"Add Zav0-9 (count of failures by first-level category) based on the contents of `Zav`\"\"\"\n",
    "\n",
    "    def map_failures(row):\n",
    "        failure_codes = str(row['Zavady']).split(',')\n",
    "        if len(failure_codes) == 0:\n",
    "            return (0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
    "        else:\n",
    "            fs = [0 for i in range(10)]\n",
    "            for failure_code in failure_codes:\n",
    "                fln = -1\n",
    "                try:\n",
    "                    fln = int(failure_code.split('.')[0])\n",
    "                except:\n",
    "                    pass\n",
    "                if fln >= 0 and fln <= 9:\n",
    "                    fs[fln] = fs[fln] + 1\n",
    "            return (fs[0], fs[1], fs[2], fs[3], fs[4], fs[5], fs[6], fs[7], fs[8], fs[9])\n",
    "\n",
    "    df[['Zav0', 'Zav1', 'Zav2', 'Zav3', 'Zav4', 'Zav5', 'Zav6', 'Zav7', 'Zav8', 'Zav9']] = df.apply(map_failures, axis=1, result_type='expand')\n",
    "    return df\n",
    "\n",
    "append_pipeline_for_all(count_failures_by_fl_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add vehicle age in days. (float to support nan)\n",
    "# TODO: Add float age in years for analysis?\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def count_vehicle_age(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    today = datetime.today()\n",
    "\n",
    "    df['StariDnu'] = df['DatPrvReg'].apply(lambda date: (today - date).days)\n",
    "\n",
    "    return df\n",
    "\n",
    "append_pipeline_for_all(count_vehicle_age)\n",
    "# apply_func_to_all(count_vehicle_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From here, preprocessing for analysis begins to mix with preprocessing just for data storage.\n",
    "\n",
    "# Process categorical indicators\n",
    "\n",
    "# 1. Gather all possible values, save them.\n",
    "cat_ind = ['STK', 'DrTP', 'TypMot', 'TZn', 'DrVoz', 'ObchOznTyp', 'Ct']\n",
    "cat_ind_vals = {cat: set() for cat in cat_ind}\n",
    "\n",
    "def gather_categorical(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    def process(row):\n",
    "        for cat in cat_ind:\n",
    "            cat_ind_vals[cat].add(row[cat])\n",
    "\n",
    "    df.apply(process, axis=1)\n",
    "    return df\n",
    "\n",
    "append_pipeline_for_all(gather_categorical)\n",
    "\n",
    "# # 2. Drop currently unusable variables.\n",
    "# def drop_useless(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     df = df.drop(['TypMot'])\n",
    "\n",
    "#     return df\n",
    "\n",
    "# 2. Convert categorical values to dummy variables\n",
    "#    Adjust this step to include only variables that are needed for analysis\n",
    "\n",
    "# 3. Convert VyslSTK, VyslEmise to an ordinal value\n",
    "#    VyslSTK:\n",
    "#      zpusobile: 0\n",
    "#      castecne zpusobile: 1\n",
    "#      nezpusobile: 2\n",
    "#    VyslEmise:\n",
    "#      vyhovuje: 0\n",
    "#      nevyhovuje: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline\n",
    "run()\n",
    "\n",
    "# Save gathered categorical values.\n",
    "import json\n",
    "with open('categories.json', \"w\") as fp:\n",
    "    json.dump({cat: list(cat_ind_vals[cat]) for cat in cat_ind}, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cat_ind_vals['STK'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
